{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-02T08:34:25.575716Z",
     "start_time": "2024-07-02T08:34:23.957410Z"
    }
   },
   "source": [
    "import bz2 # 用于解压文件\n",
    "from collections import Counter # 用于统计词频\n",
    "import re # 正则表达式\n",
    "import nltk # 文本预处理\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:34:25.582117Z",
     "start_time": "2024-07-02T08:34:25.577726Z"
    }
   },
   "cell_type": "code",
   "source": "# 下载并解压数据集",
   "id": "eff9266c79527013",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:35:51.766858Z",
     "start_time": "2024-07-02T08:34:25.584130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_file = bz2.BZ2File('D:\\DataProject\\pytorch\\data/train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('D:\\DataProject\\pytorch\\data/test.ft.txt.bz2')\n",
    "train_file = train_file.readlines()\n",
    "test_file = test_file.readlines()\n",
    "print(train_file[0])"
   ],
   "id": "e9d4096ad6ae88f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\\n'\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "从上面打印的数据可以看到，每条数据由两部分组成，*Label*和*Data*。其中：\n",
    "\n",
    "- `__label__1` 代表差评，之后将其编码为0\n",
    "- `__label__2` 代表好评，之后将其编码为1\n",
    "\n",
    "由于数据量太大，所以这里只取100w条记录进行训练，训练集和测试集按照*8:2*进行拆分："
   ],
   "id": "ed755b2aeeeee0f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:35:52.783629Z",
     "start_time": "2024-07-02T08:35:51.767865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_train = 800000\n",
    "num_test = 200000\n",
    "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
    "test_file = [x.decode('utf-8') for x in test_file[:num_test]]"
   ],
   "id": "144a80fc9f797076",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这里使用decode('utf-8')是因为源文件是以二进制类型存储的，从上面的`b''`可以看出\n",
    "\n",
    "源文件中，数据和标签是在一起的，所以要将其拆分开："
   ],
   "id": "965e190f473ccfe9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:35:57.777923Z",
     "start_time": "2024-07-02T08:35:52.786172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将__label__1替换为0，__label__2替换为1\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
    "\"\"\"\n",
    "`split(' ', 1)[1]`：将label和data分开后，获取data部分\n",
    "`[:-1]`：去掉最后一个字符(\\n)\n",
    "`lower()`: 将其转换为小写，因为区分大小写对情感识别帮助不大，且会增加编码难度\n",
    "\"\"\"\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]"
   ],
   "id": "e7f731ff93285042",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在对数据拆分后，对数据进行简单的数据清理：\n",
    "\n",
    "由于数字对情感分类帮助不大，所以这里将所有的数字都转换为0："
   ],
   "id": "f1dda418bf00737f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:36:01.567008Z",
     "start_time": "2024-07-02T08:35:57.780931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d', '0', train_sentences[i])\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d', '0', test_sentences[i])"
   ],
   "id": "aa6cc9bd0ef8c30d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "数据集中还存在包含网站的样本，例如：`Welcome to our website: www.pohabo.com`。对于这种带有网站的样本，网站地址会干扰数据处理，所以一律处理成：`Welcome to our website: <url>`：",
   "id": "b70c3df5dcb286e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:36:03.233652Z",
     "start_time": "2024-07-02T08:36:01.567607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ],
   "id": "59d27af119ff0d67",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "数据清理结束后，我们需要将**文本进行分词**，并**将仅出现一次的单词丢掉**，因为它们参考价值不大：",
   "id": "4bc55c577d5acab4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:36:25.671746Z",
     "start_time": "2024-07-02T08:36:03.235540Z"
    }
   },
   "cell_type": "code",
   "source": "nltk.download('punkt') # 使用nltk.work_tokenize前，需要下载`punkt`",
   "id": "84cd487bbc828ef1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:45:07.217378Z",
     "start_time": "2024-07-02T08:38:51.971049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = Counter()\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    words_list = nltk.word_tokenize(sentence)\n",
    "    words.update(words_list)\n",
    "    train_sentences[i] = words_list\n",
    "    \n",
    "    if i % 20000 == 0:\n",
    "        print(str((i * 100) / num_train) + '% done')\n",
    "print('100% done')"
   ],
   "id": "6accfd3fd292cf88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "2.5% done\n",
      "5.0% done\n",
      "7.5% done\n",
      "10.0% done\n",
      "12.5% done\n",
      "15.0% done\n",
      "17.5% done\n",
      "20.0% done\n",
      "22.5% done\n",
      "25.0% done\n",
      "27.5% done\n",
      "30.0% done\n",
      "32.5% done\n",
      "35.0% done\n",
      "37.5% done\n",
      "40.0% done\n",
      "42.5% done\n",
      "45.0% done\n",
      "47.5% done\n",
      "50.0% done\n",
      "52.5% done\n",
      "55.0% done\n",
      "57.5% done\n",
      "60.0% done\n",
      "62.5% done\n",
      "65.0% done\n",
      "67.5% done\n",
      "70.0% done\n",
      "72.5% done\n",
      "75.0% done\n",
      "77.5% done\n",
      "80.0% done\n",
      "82.5% done\n",
      "85.0% done\n",
      "87.5% done\n",
      "90.0% done\n",
      "92.5% done\n",
      "95.0% done\n",
      "97.5% done\n",
      "100% done\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "移除仅出现一次的单词",
   "id": "def04390b1301856"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:45:49.451486Z",
     "start_time": "2024-07-02T08:45:49.180894Z"
    }
   },
   "cell_type": "code",
   "source": "words = {k:v for k, v in words.items() if v > 1}",
   "id": "288e7f0eba5bb7a0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "将words按照出现次数由大到小排序，并转换为list，**作为我们的词典**，之后**对于单词的编码会基于该词典**：",
   "id": "709a379357352d6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:46:43.086142Z",
     "start_time": "2024-07-02T08:46:42.975469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = sorted(words, key = words.get, reverse = True)\n",
    "print(words[:10])"
   ],
   "id": "9458974f78fa1025",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'the', ',', 'i', 'and', 'a', 'to', 'it', 'of', 'this']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "向词典中增加一个单词：\n",
    "\n",
    "- `_PAD`：表示填充，因为后续会固定所有句子长度。过长的句子进行阶段，过短的句子使用该单词进行填充"
   ],
   "id": "101c2e505c729753"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:47:08.856560Z",
     "start_time": "2024-07-02T08:47:08.828503Z"
    }
   },
   "cell_type": "code",
   "source": "words = ['_PAD'] + words",
   "id": "df99a35bb1187ae2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "整理好词典后，对**单词进行编码**，即**将单词映射成数字**，这里直接使用单词所在的数字下表作为单词的编码值：",
   "id": "5cac845059ed65c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:52:10.293249Z",
     "start_time": "2024-07-02T08:52:10.036544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word2idx = {o:i for i, o in enumerate(words)}\n",
    "idx2word = {i:o for i, o in enumerate(words)}"
   ],
   "id": "c344ab280f3c9ba5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "映射字典准备完毕后，就可以将`train_sentences`中存储的单词转化为数字了：",
   "id": "a1d05983efb80dcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:53:08.816606Z",
     "start_time": "2024-07-02T08:52:35.168387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, sentence in enumerate(train_sentences):\n",
    "    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    test_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]"
   ],
   "id": "6c9cff3105b75b28",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> 上面的`else 0`表示：如果单词没有在字典中出现过，则使用编码0，对应上面的`_PAD`\n",
    "\n",
    "为了方便构建模型，需要固定所有句子的长度，这里选择200作为句子的固定长度，对于长度不够的句子，在前面填充`0`(`_PAD`)，超出长度的句子进行从后面截断："
   ],
   "id": "22039a22046619dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:57:54.378869Z",
     "start_time": "2024-07-02T08:57:41.772805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_input(sentences, seq_len):\n",
    "    \"\"\"\n",
    "    将句子长度固定为`seq_len`，超出长度的从后面阶段，长度不足的在前面补0\n",
    "    \"\"\"\n",
    "    features = np.zeros((len(sentences), seq_len), dtype = int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "# 固定测试数据集和训练数据集的句子长度\n",
    "train_sentences = pad_input(train_sentences, 200)\n",
    "test_sentences = pad_input(test_sentences, 200)"
   ],
   "id": "edc71c00813aa1fc",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "上述方法除了固定长度外，还顺便将数字转化为了numpy数组。Label数据集也需要转换一下：",
   "id": "a2da7348b66cce31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:58:03.612563Z",
     "start_time": "2024-07-02T08:58:03.547460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ],
   "id": "28b31ee96369ff17",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 构建模型",
   "id": "5493ab20e2f6ef03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:43:32.397087Z",
     "start_time": "2024-07-02T09:43:32.392453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn"
   ],
   "id": "c5f1ca824254c2ed",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "构建训练数据集和测试数据集的DataLoader，同时**定义BatchSize为200**:",
   "id": "20b9813b3bb001dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:43:33.362138Z",
     "start_time": "2024-07-02T09:43:33.355611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 200\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
    "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle = True, batch_size = batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle = True, batch_size = batch_size)"
   ],
   "id": "bced989f6317a54b",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:43:34.100755Z",
     "start_time": "2024-07-02T09:43:34.096151Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")",
   "id": "da9572742e9c3077",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:43:35.019832Z",
     "start_time": "2024-07-02T09:43:35.005259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.n_layers = n_layers = 2 # LSTM的层数\n",
    "        self.hidden_dim = hidden_dim = 512 # LSTM的隐藏层大小\n",
    "        embedding_dim = 400 # 将单词编码为400的向量\n",
    "        drop_prob= 0.5 # dropout的概率\n",
    "        \n",
    "        # 定义embedding， 负责将数字编码称向量\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM (embedding_dim, # 输入的维度\n",
    "                            hidden_dim, # LSTM输出的hidden_state的维度\n",
    "                            n_layers, # LSTM的层数\n",
    "                            dropout=drop_prob,\n",
    "                            batch_first=True # 第一个维度是否是batch_size\n",
    "                            )\n",
    "        \n",
    "        # LSTM结束后的全连接线性层\n",
    "        self.fc = nn.Linear(in_features = hidden_dim, # 将LSTM输出作为线性层的输入\n",
    "                            out_features = 1) # 由于情感分析只需要输出0或1，所以输出维度为1\n",
    "        self.sigmoid = nn.Sigmoid() # 线性层输出后，还需要过一下sigmoid\n",
    "        # 给最后的全连接层加一个dropout\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        x: 本次的输入，其size为(batch_size, 200)，200为句子长度\n",
    "        hidden: 上一时刻的Hidden State和Cell State。类型为tuple: (h, c),\n",
    "        其中h和c的size都为(n_layers, batch_size, hidden_dim), 即(2, 200, 512)\n",
    "        \"\"\"\n",
    "        # 因为一次输入一组数据，所以第一个维度是batch的大小\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 由于embedding只接受LongTensor类型，所以将x转换为LongTensor类型\n",
    "        x = x.long()\n",
    "        \n",
    "         # 对x进行编码，这里会将x的size由(batch_size, 200)转化为(batch_size, 200, embedding_dim)\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        # 将编码后的向量和上一时刻的hidden_state传给LSTM，并获取本次的输出和隐状态（hidden_state, cell_state）\n",
    "        # lstm_out的size为 (batch_size, 200, 512)，200是单词的数量，由于是一个单词一个单词送给LSTM的，所以会产生与单词数量相同的输出\n",
    "        # hidden为tuple(hidden_state, cell_state)，它们俩的size都为(2, batch_size, 512), 2是由于lstm有两层。由于是所有单词都是共享隐状态的，所以并不会出现上面的那个200\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # 接下来要过全连接层，所以size变为(batch_size * 200, hidden_dim)，\n",
    "        # 之所以是batch_size * 200=40000，是因为每个单词的输出都要经过全连接层。\n",
    "        # 换句话说，全连接层的batch_size为40000\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # 给全连接层加个Dropout\n",
    "        out = self.dropout(lstm_out)\n",
    "        \n",
    "        # 将dropout后的数据送给全连接层\n",
    "        # 全连接层输出的size为(40000, 1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # 过一下sigmoid\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        # 将最终的输出维度变为（batch_size, 200），即每个单词都对应一个输出\n",
    "        out = out.view(batch_size, -1)\n",
    "        \n",
    "        # 只去最后一个单词的输出\n",
    "        # 所以out的size会变为(200, 1)\n",
    "        out = out[:, -1]\n",
    "        \n",
    "        # 将输出和本次的(h, c)返回\n",
    "        return out, hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        初始化隐状态：第一次送给LSTM时，没有隐状态，所以要初始化一个\n",
    "        这里的初始化策略是全部赋0。\n",
    "        这里之所以是tuple，是因为LSTM需要接受两个隐状态hidden state和cell state\n",
    "        \"\"\"\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
    "                  torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
    "        return hidden"
   ],
   "id": "4e3c6ba899d3bef0",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "构建模型对象",
   "id": "2326294ba703e55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:43:36.548398Z",
     "start_time": "2024-07-02T09:43:35.895457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SentimentNet(len(words))\n",
    "model.to(device)"
   ],
   "id": "45d6161c7cd5e978",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentNet(\n",
       "  (embedding): Embedding(221521, 400)\n",
       "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "定义损失函数",
   "id": "49db4835cf5952f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:43:37.910049Z",
     "start_time": "2024-07-02T09:43:37.905430Z"
    }
   },
   "cell_type": "code",
   "source": "criterion = nn.BCELoss()",
   "id": "288105916af66385",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:43:38.601472Z",
     "start_time": "2024-07-02T09:43:38.596970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr = 0.005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ],
   "id": "aff426a81458cf69",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "定义训练代码",
   "id": "fd9f1a68f94ae7a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epochs = 2 # 一共训练两轮\n",
    "counter = 0 # 用于记录训练次数\n",
    "print_every = 1000 # 每1000次打印一下当前状态\n",
    "\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size) # 初始化第一个Hidden_state\n",
    "    \n",
    "    for inputs, labels in train_loader:  # 从train_loader中获取一组inputs和labels\n",
    "        counter += 1# 训练次数+1\n",
    "         \n",
    "        # 将上次输出的hidden_state转为tuple格式\n",
    "        # 因为有两次，所以len(h)==2\n",
    "        h = tuple([e.data for e in h])\n",
    "        \n",
    "        # 将数据迁移到GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 清空模型梯度\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 将本轮的输入和hidden_state送给模型，进行前向传播，\n",
    "        # 然后获取本次的输出和新的hidden_state\n",
    "        output, h = model(inputs, h)\n",
    "        \n",
    "        # 将预测值和真实值送给损失函数计算损失\n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        # 进行反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 对模型进行裁剪，防止模型梯度爆炸\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 5)\n",
    "        \n",
    "        # 更新权重\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 隔一定次数打印一下当前状态\n",
    "        if counter % print_every == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()))"
   ],
   "id": "930c6662d03c3174",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> 如果这里抛出了`RuntimeError: CUDA out of memory. Tried to allocate ...`异常，可以将batch_size调小，或者清空gpu中的缓存（`torch.cuda.empty_cache()`）",
   "id": "6ea44dafd04ff91a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "经过一段时间的训练，现在来评估一下模型的性能：",
   "id": "27e9e4e57eb6c6e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_losses = [] # 记录测试数据集的损失\n",
    "num_correct = 0 # 记录预测正确的数量\n",
    "h = model.init_hidden(batch_size) # 初始化hidden_state和cell_state\n",
    "model.eval() # 切换到评估模式\n",
    "\n",
    "# 开始评估模型\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, h = model(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.rount(output.squeeze()) # 将模型四舍五入为0和1\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred)) # 计算预测正确的数据\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct / len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc * 100))"
   ],
   "id": "4b45a1764eb3e1f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "最终，经过训练后，可以得到90%以上的准确率。\n",
    "\n",
    "我们来实际尝试一下，定义一个`predict(sentence)`函数，输入一个句子，输出其预测结果："
   ],
   "id": "c06fc2c2277f7a41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict(sentence):\n",
    "    # 将句子分词后，转换为数字\n",
    "    sentences = [[word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]]\n",
    "    \n",
    "    # 将句子变为固定长度200\n",
    "    \n",
    "    sentences = pad_input(sentences, 200)\n",
    "    \n",
    "    # 将数据转移到GPU中\n",
    "    sentences - torch.Tensor(sentences).long().to(device)\n",
    "    \n",
    "    # 初始化隐状态\n",
    "    h = (torch.Tensor(2, 1, 512).zero_().to(device),\n",
    "         torch.Tensor(2, 1, 512).zero_().to(device))\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    # 预测\n",
    "    if model(sentences, h)[0] > 0.5:\n",
    "        print(\"positive\")\n",
    "    else:\n",
    "        print(\"negative\")"
   ],
   "id": "caf9c797e42ffa7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "predict(\"The film is so boring\")\n",
    "predict(\"The actor is too ugly.\")"
   ],
   "id": "dfaed68761ce12ce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python(ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
